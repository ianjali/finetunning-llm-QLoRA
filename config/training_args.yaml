bit_bytes_params:
  # Activate 4-bit precision base model loading
  use_4bit: True
  # Compute dtype for 4-bit base models
  bnb_4bit_compute_dtype: "float16"
  # Quantization type (fp4 or nf4)
  bnb_4bit_quant_type: "nf4"
  # Activate nested quantization for 4-bit base models (double quantization)
  use_nested_quant: False

QLoRA_params:
  # LoRA attention dimension
  lora_r: 64
  # Alpha parameter for LoRA scaling
  lora_alpha: 16
  # Dropout probability for LoRA layers
  lora_dropout: 0.1

model_params:
  model_name: "NousResearch/Llama-2-7b-chat-hf"
  output_dir: "./results"
  fine_tune_model: "model/finetuned_model/Llama-2-7b-chat-finetune"

training_params:
  # Number of training epochs
  num_train_epochs: 1
  # Enable fp16/bf16 training (set bf16 to True with an A100)
  fp16: false 
  bf16: false 
  # Batch size per GPU for training and evaluation
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  # Number of update steps to accumulate the gradients for
  gradient_accumulation_steps: 1
  # Enable gradient checkpointing
  gradient_checkpointing: true
  # Maximum gradient normal (gradient clipping)
  max_grad_norm: 0.3
  # Initial learning rate (AdamW optimizer)
  learning_rate: 2e-4
  # Weight decay to apply to all layers except bias/LayerNorm weights
  weight_decay: 0.001
  # Optimizer to use
  optim: "paged_adamw_32bit"
  # Learning rate schedule
  lr_scheduler_type: "cosine"
  # Number of training steps (overrides num_train_epochs)
  max_steps: -1
  # Ratio of steps for a linear warmup (from 0 to learning rate)
  warmup_ratio: 0.03
  # Group sequences into batches with same length
  # Saves memory and speeds up training considerably
  group_by_length: true
  # Save checkpoint every X updates steps
  save_steps: 0
  # Log every X updates steps
  logging_steps: 25

SFT_params:
  # Maximum sequence length to use
  max_seq_length: null
  # Pack multiple short examples in the same input sequence to increase efficiency
  packing: false


